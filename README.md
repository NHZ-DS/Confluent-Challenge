üè≠ HSE Sentinel: Predictive Industrial IoT & AI Safety Pipeline
<div align="left"> <img src="https://img.shields.io/badge/Python-3.11-3776AB?style=for-the-badge&logo=python&logoColor=white" /> <img src="https://img.shields.io/badge/Framework-FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white" /> <img src="https://img.shields.io/badge/Frontend-React-61DAFB?style=for-the-badge&logo=react&logoColor=black" /> <img src="https://img.shields.io/badge/AI-Gemini_2.0-4285F4?style=for-the-badge&logo=google-gemini&logoColor=white" /> <img src="https://img.shields.io/badge/Streaming-Confluent_Kafka-000000?style=for-the-badge&logo=confluent&logoColor=white" /> <img src="https://img.shields.io/badge/Infrastructure-Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white" /> </div>

üìñ Overview

HSE Sentinel is a high-performance, real-time industrial monitoring platform. It bridges the gap between raw IoT telemetry and actionable safety intelligence by combining Confluent Cloud (Kafka & Flink) with Google Vertex AI and Gemini 2.0.

The system doesn't just monitor data; it predicts hazards and provides expert HSE guidance in real-time.

üöÄ Advanced Infrastructure

Our architecture is designed for low-latency decision-making and enterprise-grade scalability:

Ingestion: A Python-based IoT simulator streaming high-frequency raw CO2 and PM2.5 data to Confluent Cloud.

Stream Processing: Flink SQL transforms raw data into stable 30-second tumbling windows to eliminate sensor noise and calculate trends.

Predictive AI (Vertex AI): A time-series model integrated into the pipeline to forecast pollutant levels 5 minutes into the future.

Generative HSE Agent (Gemini 2.0): Google's latest LLM analyzes live trends to generate immediate, context-aware safety recommendations.

Real-Time Delivery: A FastAPI backend pushes data to a React dashboard via WebSockets for instant visualization.

üõ†Ô∏è Tech Stack

Layer	Technologies

Streaming	Confluent Cloud, Kafka, Flink SQL

Artificial Intelligence	Google Gemini 2.0 Flash-Lite, Vertex AI (Forecasting)

Backend	Python 3.11, FastAPI, Asynchronous Kafka Consumers

Frontend	React 18, Vite, Tailwind CSS v4, Recharts, Lucide Icons

Infrastructure	Docker, Nginx (Reverse Proxy), .dockerignore

Exporter vers Sheets

üìà Roadmap & Current Progress

[x] Phase 1: Real-time Python Producer (Sensor Simulation) with sinusoidal patterns.

[x] Phase 2: Flink SQL aggregation for data stability.

[x] Phase 3: Vertex AI integration for predictive severity analysis.

[x] Phase 4: Gemini 2.0 HSE Expert Tip integration.

[x] Phase 5: Full-stack containerization with Docker & Nginx.

‚öôÔ∏è Setup & Installation

Note: For security, .env and .venv files are excluded. You must provide your own credentials.

1. Environment Variables

Create a .env file in the root directory:

Plaintext

# Confluent Kafka

KAFKA_BOOTSTRAP_SERVER=your_server

KAFKA_API_KEY=your_key

KAFKA_API_SECRET=your_secret

# Google Cloud

GOOGLE_APPLICATION_CREDENTIALS=path_to_your_service_account.json

# Frontend Environment Variables
Create a .env file inside the frontend/ directory.
Add the following variables:  

VITE_WS_TARGET=your_livedata_websocket_endpoint   
VITE_WS_FORECAST=your_forecast_websocket_endpoint  

# Backend Kafka Credentials
For security reasons, Kafka credentials are not committed to the repository.  
Navigate to the backend configuration file: backend/client.properties  
Replace all placeholder fields (bootstrap server, API key, secret, etc.) with your own Confluent Cloud credentials.  

2. Using Docker (Recommended)

The fastest way to launch the entire stack (Backend + Frontend + Proxy):

Bash

# Build and start all services

docker-compose up --build

# Run in detached mode

docker-compose up -d

3. Manual Installation
   
Backend:

Bash

cd backend

pip install -r requirements.txt

uvicorn main:app --host 0.0.0.0 --port 8000

Frontend:

Bash

cd frontend

npm install

npm run dev

üõ°Ô∏è Security & Reliability

Graceful Shutdown: Backend uses asynccontextmanager to ensure Kafka consumers close connections properly.

Data Integrity: Flink SQL ensures that spikes are validated over 30s windows to prevent false positive alerts.

Production Ready: Nginx handles static file serving and reverse-proxying to ensure high availability.

üß™Mock Frontend:  

To facilitate easy demonstration and UI testing without relying on live backend services, a mock frontend has been added to this repository.  

mockFrontend/  

This folder contains a standalone version of the frontend that:  

Uses mocked, pattern-based data generation instead of WebSocket streams  

Simulates realistic pollution data using sine/cosine wave patterns  

Includes mock AI-based recommendations that emulate the behavior of the real AI engine  

The mock frontend has been deployed using Vercel and can be accessed here: https://confluent-challenge.vercel.app/ 



